{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8762caf407794ad4a2a3d35586b8da41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83567946faf340aa9aacad1fb5716d2f",
              "IPY_MODEL_2ebad19c5e8b417ab3ae6d8057d70725",
              "IPY_MODEL_83acff53ee91485885a8acc85ce25742",
              "IPY_MODEL_0a36d4e8036647cd93b8b6380674ada0",
              "IPY_MODEL_7e0fe423eb39405abfb424aabedc265f"
            ],
            "layout": "IPY_MODEL_cdbd68c4e4574ebf931159fb0b22b4e0"
          }
        },
        "83567946faf340aa9aacad1fb5716d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_966a1e7f4e3b466fb02ff09a85e25241",
            "placeholder": "​",
            "style": "IPY_MODEL_a2270864433042feac3b767927db4c2a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2ebad19c5e8b417ab3ae6d8057d70725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e1522862370c47a89fc987702b2af4d7",
            "placeholder": "​",
            "style": "IPY_MODEL_f2464cf775cb43cb81414dade5743474",
            "value": ""
          }
        },
        "83acff53ee91485885a8acc85ce25742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_940aee2ae8bb49f093d128d13e880b39",
            "style": "IPY_MODEL_dbdfc30a750d4911b4bd0fa390bb94de",
            "value": true
          }
        },
        "0a36d4e8036647cd93b8b6380674ada0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_09f8f260bd5a44809031d79829a7a2f6",
            "style": "IPY_MODEL_aa64c199d6bb442db4f7ec93cc9f9c88",
            "tooltip": ""
          }
        },
        "7e0fe423eb39405abfb424aabedc265f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5018765cb9cf473a876d84276be542d9",
            "placeholder": "​",
            "style": "IPY_MODEL_d7a39e48ee794a8a8bb28f4d7830a754",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "cdbd68c4e4574ebf931159fb0b22b4e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "966a1e7f4e3b466fb02ff09a85e25241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2270864433042feac3b767927db4c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1522862370c47a89fc987702b2af4d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2464cf775cb43cb81414dade5743474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "940aee2ae8bb49f093d128d13e880b39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbdfc30a750d4911b4bd0fa390bb94de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09f8f260bd5a44809031d79829a7a2f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa64c199d6bb442db4f7ec93cc9f9c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5018765cb9cf473a876d84276be542d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a39e48ee794a8a8bb28f4d7830a754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is2FihksBdXO",
        "outputId": "e9e91f11-37fc-48e5-fcb7-a7a0b63416ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============NVSMI LOG==============\n",
            "\n",
            "Timestamp                                 : Wed Apr 19 21:33:39 2023\n",
            "Driver Version                            : 525.85.12\n",
            "CUDA Version                              : 12.0\n",
            "\n",
            "Attached GPUs                             : 1\n",
            "GPU 00000000:00:04.0\n",
            "    Product Name                          : Tesla T4\n",
            "    Product Brand                         : NVIDIA\n",
            "    Product Architecture                  : Turing\n",
            "    Display Mode                          : Enabled\n",
            "    Display Active                        : Disabled\n",
            "    Persistence Mode                      : Disabled\n",
            "    MIG Mode\n",
            "        Current                           : N/A\n",
            "        Pending                           : N/A\n",
            "    Accounting Mode                       : Disabled\n",
            "    Accounting Mode Buffer Size           : 4000\n",
            "    Driver Model\n",
            "        Current                           : N/A\n",
            "        Pending                           : N/A\n",
            "    Serial Number                         : 1654321009282\n",
            "    GPU UUID                              : GPU-9e159329-0cfe-4185-1bc4-ce75d07995ec\n",
            "    Minor Number                          : 0\n",
            "    VBIOS Version                         : 90.04.A7.00.01\n",
            "    MultiGPU Board                        : No\n",
            "    Board ID                              : 0x4\n",
            "    Board Part Number                     : 900-2G183-6300-T00\n",
            "    GPU Part Number                       : 1EB8-895-A1\n",
            "    Module ID                             : 1\n",
            "    Inforom Version\n",
            "        Image Version                     : G183.0200.00.02\n",
            "        OEM Object                        : 1.1\n",
            "        ECC Object                        : 5.0\n",
            "        Power Management Object           : N/A\n",
            "    GPU Operation Mode\n",
            "        Current                           : N/A\n",
            "        Pending                           : N/A\n",
            "    GSP Firmware Version                  : N/A\n",
            "    GPU Virtualization Mode\n",
            "        Virtualization Mode               : Pass-Through\n",
            "        Host VGPU Mode                    : N/A\n",
            "    IBMNPU\n",
            "        Relaxed Ordering Mode             : N/A\n",
            "    PCI\n",
            "        Bus                               : 0x00\n",
            "        Device                            : 0x04\n",
            "        Domain                            : 0x0000\n",
            "        Device Id                         : 0x1EB810DE\n",
            "        Bus Id                            : 00000000:00:04.0\n",
            "        Sub System Id                     : 0x12A210DE\n",
            "        GPU Link Info\n",
            "            PCIe Generation\n",
            "                Max                       : 3\n",
            "                Current                   : 1\n",
            "                Device Current            : 1\n",
            "                Device Max                : 3\n",
            "                Host Max                  : N/A\n",
            "            Link Width\n",
            "                Max                       : 16x\n",
            "                Current                   : 16x\n",
            "        Bridge Chip\n",
            "            Type                          : N/A\n",
            "            Firmware                      : N/A\n",
            "        Replays Since Reset               : 0\n",
            "        Replay Number Rollovers           : 0\n",
            "        Tx Throughput                     : 0 KB/s\n",
            "        Rx Throughput                     : 0 KB/s\n",
            "        Atomic Caps Inbound               : N/A\n",
            "        Atomic Caps Outbound              : N/A\n",
            "    Fan Speed                             : N/A\n",
            "    Performance State                     : P8\n",
            "    Clocks Throttle Reasons\n",
            "        Idle                              : Active\n",
            "        Applications Clocks Setting       : Not Active\n",
            "        SW Power Cap                      : Not Active\n",
            "        HW Slowdown                       : Not Active\n",
            "            HW Thermal Slowdown           : Not Active\n",
            "            HW Power Brake Slowdown       : Not Active\n",
            "        Sync Boost                        : Not Active\n",
            "        SW Thermal Slowdown               : Not Active\n",
            "        Display Clock Setting             : Not Active\n",
            "    FB Memory Usage\n",
            "        Total                             : 15360 MiB\n",
            "        Reserved                          : 258 MiB\n",
            "        Used                              : 0 MiB\n",
            "        Free                              : 15101 MiB\n",
            "    BAR1 Memory Usage\n",
            "        Total                             : 256 MiB\n",
            "        Used                              : 2 MiB\n",
            "        Free                              : 254 MiB\n",
            "    Compute Mode                          : Default\n",
            "    Utilization\n",
            "        Gpu                               : 0 %\n",
            "        Memory                            : 0 %\n",
            "        Encoder                           : 0 %\n",
            "        Decoder                           : 0 %\n",
            "    Encoder Stats\n",
            "        Active Sessions                   : 0\n",
            "        Average FPS                       : 0\n",
            "        Average Latency                   : 0\n",
            "    FBC Stats\n",
            "        Active Sessions                   : 0\n",
            "        Average FPS                       : 0\n",
            "        Average Latency                   : 0\n",
            "    Ecc Mode\n",
            "        Current                           : Enabled\n",
            "        Pending                           : Enabled\n",
            "    ECC Errors\n",
            "        Volatile\n",
            "            SRAM Correctable              : 0\n",
            "            SRAM Uncorrectable            : 0\n",
            "            DRAM Correctable              : 0\n",
            "            DRAM Uncorrectable            : 0\n",
            "        Aggregate\n",
            "            SRAM Correctable              : 0\n",
            "            SRAM Uncorrectable            : 0\n",
            "            DRAM Correctable              : 0\n",
            "            DRAM Uncorrectable            : 0\n",
            "    Retired Pages\n",
            "        Single Bit ECC                    : 0\n",
            "        Double Bit ECC                    : 0\n",
            "        Pending Page Blacklist            : No\n",
            "    Remapped Rows                         : N/A\n",
            "    Temperature\n",
            "        GPU Current Temp                  : 66 C\n",
            "        GPU T.Limit Temp                  : N/A\n",
            "        GPU Shutdown Temp                 : 96 C\n",
            "        GPU Slowdown Temp                 : 93 C\n",
            "        GPU Max Operating Temp            : 85 C\n",
            "        GPU Target Temperature            : N/A\n",
            "        Memory Current Temp               : N/A\n",
            "        Memory Max Operating Temp         : N/A\n",
            "    Power Readings\n",
            "        Power Management                  : Supported\n",
            "        Power Draw                        : 16.98 W\n",
            "        Power Limit                       : 70.00 W\n",
            "        Default Power Limit               : 70.00 W\n",
            "        Enforced Power Limit              : 70.00 W\n",
            "        Min Power Limit                   : 60.00 W\n",
            "        Max Power Limit                   : 70.00 W\n",
            "    Clocks\n",
            "        Graphics                          : 300 MHz\n",
            "        SM                                : 300 MHz\n",
            "        Memory                            : 405 MHz\n",
            "        Video                             : 540 MHz\n",
            "    Applications Clocks\n",
            "        Graphics                          : 585 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "    Default Applications Clocks\n",
            "        Graphics                          : 585 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "    Deferred Clocks\n",
            "        Memory                            : N/A\n",
            "    Max Clocks\n",
            "        Graphics                          : 1590 MHz\n",
            "        SM                                : 1590 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "        Video                             : 1470 MHz\n",
            "    Max Customer Boost Clocks\n",
            "        Graphics                          : 1590 MHz\n",
            "    Clock Policy\n",
            "        Auto Boost                        : N/A\n",
            "        Auto Boost Default                : N/A\n",
            "    Voltage\n",
            "        Graphics                          : N/A\n",
            "    Fabric\n",
            "        State                             : N/A\n",
            "        Status                            : N/A\n",
            "    Processes                             : None\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.9/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from trl) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.9/dist-packages (from trl) (1.22.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (from trl) (0.18.0)\n",
            "Requirement already satisfied: transformers>=4.18.0 in /usr/local/lib/python3.9/dist-packages (from trl) (4.28.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from trl) (2.0.0+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->trl) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->trl) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->trl) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->trl) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->trl) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->trl) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->trl) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->trl) (3.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.18.0->trl) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.18.0->trl) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.18.0->trl) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.18.0->trl) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers>=4.18.0->trl) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.18.0->trl) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.18.0->trl) (0.13.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (2023.4.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (0.18.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (1.5.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (9.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->trl) (0.70.14)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->trl) (23.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->trl) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->trl) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->trl) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->trl) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->trl) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.18.0->trl) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.18.0->trl) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.18.0->trl) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->trl) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.9/dist-packages (0.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# ATTIS 2023 Tutorial on RLHF\n",
        "# Original source code for this demo was taken from https://github.com/lvwerra/trl/blob/main/examples/toxicity/scripts/gpt-j-6b-toxicity.py\n",
        "# This code was adjusted for a tutorial with some extra additions/tweaks and the ability to run in Colab on a standard GPU \n",
        "# This script will complete, but\n",
        "\n",
        "# Make sure your NoteBook Settings are using a GPU\n",
        "!nvidia-smi -q\n",
        "\n",
        "# Load required libraries\n",
        "!pip install transformers\n",
        "!pip install trl\n",
        "!pip install dataclasses\n",
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        ")\n",
        "\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed\n",
        "from trl.core import LengthSampler\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "DHeBb5f-BlBi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################################################\n",
        "# This is a fully working simple example to use trl with accelerate.\n",
        "#\n",
        "# This example fine-tunes a GPTJ model to generate less toxic contents\n",
        "# by using allenai/real-toxicity-prompts dataset. We use PPO\n",
        "#  (proximal policy optimization) to optimize the model.\n",
        "# in any of the following settings (with the same script):\n",
        "#   - single CPU or single GPU\n",
        "#   - multi GPUS (using PyTorch distributed mode)\n",
        "#   - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)\n",
        "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
        "#\n",
        "# To run it in each of these various modes, first initialize the accelerate\n",
        "# configuration with `accelerate config`\n",
        "#\n",
        "########################################################################\n",
        "\n",
        "\n",
        "# We first define the configuration of the experiment, defining the model, the dataset,\n",
        "# the training parameters, and the PPO parameters.\n",
        "# Check the default arguments in the `PPOConfig` class for more details.\n",
        "# If you want to log with tensorboard, add the kwarg\n",
        "# `accelerator_kwargs={\"logging_dir\": PATH_TO_LOGS}` to the PPOConfig.\n",
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    \"\"\"\n",
        "    The name of the Casual LM model we wish to fine with PPO\n",
        "    \"\"\"\n",
        "\n",
        "    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode\n",
        "    # models like gpt-neo* models are more suitable.\n",
        "    model_name: Optional[str] = field(default=\"EleutherAI/gpt-neo-1.3B\", metadata={\"help\": \"the model name\"})\n",
        "    #model_name: Optional[str] = field(default=\"EleutherAI/gpt-neo-125M\", metadata={\"help\": \"the model name\"})\n",
        "    #model_name: Optional[str] = field(default=\"ybelkada/gpt-j-6b-sharded-bf16\", metadata={\"help\": \"the model name\"})\n",
        "    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
        "    learning_rate: Optional[float] = field(default=(1.47e-5) * 2, metadata={\"help\": \"the learning rate\"})\n",
        "    mini_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"the PPO minibatch size\"})\n",
        "    #batch_size: Optional[int] = field(default=16, metadata={\"help\": \"the batch size\"})\n",
        "    #batch_size: Optional[int] = field(default=64, metadata={\"help\": \"the batch size\"}) #125M model\n",
        "    batch_size: Optional[int] = field(default=16, metadata={\"help\": \"the batch size\"}) #1.3B model\n",
        "    gradient_accumulation_steps: Optional[int] = field(\n",
        "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
        "    )\n",
        "    model_save_path: Optional[str] = field(\n",
        "        #default=\"./gpt-j-6B-detoxified-long-context-26-shl-1e4-final\",\n",
        "        #default=\"./gpt-jneo-125M-detoxified-final\",\n",
        "        default=\"./gpt-jneo-1.3B-detoxified-final\",\n",
        "        metadata={\"help\": \"the path to save the model\"},\n",
        "    )\n",
        "\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args, outputargs = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
        "pprint(script_args) # Capturing unused -f argument added by Colab?\n",
        "\n",
        "config = PPOConfig(\n",
        "    seed=42,\n",
        "    model_name=script_args.model_name,\n",
        "    learning_rate=script_args.learning_rate,\n",
        "    log_with=script_args.log_with,\n",
        "    ppo_epochs=100,\n",
        "    mini_batch_size=script_args.mini_batch_size,\n",
        "    batch_size=script_args.batch_size,\n",
        "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYKu-kcXE9sC",
        "outputId": "02160b3b-2dd8-4753-8ef4-c36e49ce44d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ScriptArguments(model_name='EleutherAI/gpt-neo-1.3B', log_with=None, learning_rate=2.94e-05, mini_batch_size=4, batch_size=16, gradient_accumulation_steps=1, model_save_path='./gpt-jneo-1.3B-detoxified-final')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is an example function to build the dataset. In our case, we use the real-toxicity-prompts dataset\n",
        "# from the `datasets` library. One should customize this function to train the model on\n",
        "# its own dataset.\n",
        "def build_dataset(\n",
        "    config, dataset_name=\"allenai/real-toxicity-prompts\", input_min_text_length=5, input_max_text_length=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
        "    customize this function to train the model on its own dataset.\n",
        "    Args:\n",
        "        dataset_name (`str`):\n",
        "            The name of the dataset to be loaded.\n",
        "    Returns:\n",
        "        dataloader (`torch.utils.data.DataLoader`):\n",
        "            The dataloader for the dataset.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    ds = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "    def filter_fn(sample):\n",
        "        toxicity = sample[\"prompt\"][\"toxicity\"]\n",
        "        return toxicity is not None and toxicity > 0.3\n",
        "    ds = ds.filter(filter_fn, batched=False)\n",
        "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        prompt = sample[\"prompt\"][\"text\"]\n",
        "        continuation = sample[\"continuation\"][\"text\"]\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt + continuation)[: input_size()]\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    ds = ds.train_test_split(test_size=0.2, shuffle=False)[\"train\"]\n",
        "\n",
        "    return ds\n",
        "\n",
        "# We retrieve the dataloader by calling the `build_dataset` function.\n",
        "min_input_length = 30\n",
        "max_input_length = 40\n",
        "dataset = build_dataset(config, input_min_text_length=min_input_length, input_max_text_length=max_input_length)\n",
        "\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "\n",
        "# set seed before initializing value head for deterministic eval\n",
        "set_seed(config.seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXlrXWfNLIMR",
        "outputId": "33252a02-3549-40b0-83ff-17ef2c017a0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-98c7dae678df6ade.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-078348aa5a3f117a.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's build the model, the reference model, and the tokenizer. We first load the model\n",
        "# in bfloat16 to save memory using `transformers`.\n",
        "model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)\n",
        "# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
        "\n",
        "# We create a reference model by sharing 20 layers  (don't want complete copy)\n",
        "#ref_model = create_reference_model(model, num_shared_layers=20)\n",
        "ref_model = create_reference_model(model, num_shared_layers=10)\n",
        "\n",
        "# We make sure to use `Adam` optimizer on the model parameters that require gradients.\n",
        "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
        "\n",
        "# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n",
        "# only for this model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config,\n",
        "    model,\n",
        "    ref_model=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset,\n",
        "    data_collator=collator,\n",
        "    optimizer=optimizer,\n",
        ")"
      ],
      "metadata": {
        "id": "N5lmtOvYLKQb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "# We then build the reward pipeline, we will use the toxicity model to compute the reward.\n",
        "# We first load the toxicity model and tokenizer.\n",
        "toxicity_model_id = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)\n",
        "# We load the toxicity model in fp16 to save memory.\n",
        "toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(\n",
        "    ppo_trainer.accelerator.device\n",
        ")\n",
        "\n",
        "# We then define the arguments to pass to the `generate` function. These arguments\n",
        "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
        "# the `generate` function of the trained model.\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "output_min_length = 20\n",
        "output_max_length = 30\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "model_save_path = script_args.model_save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ZWacygN034",
        "outputId": "b543f88d-e928-480f-a160-3a30364ac6d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
            "Wall time: 8.11 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You will need to have a hugging face account and an API key\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "8762caf407794ad4a2a3d35586b8da41",
            "83567946faf340aa9aacad1fb5716d2f",
            "2ebad19c5e8b417ab3ae6d8057d70725",
            "83acff53ee91485885a8acc85ce25742",
            "0a36d4e8036647cd93b8b6380674ada0",
            "7e0fe423eb39405abfb424aabedc265f",
            "cdbd68c4e4574ebf931159fb0b22b4e0",
            "966a1e7f4e3b466fb02ff09a85e25241",
            "a2270864433042feac3b767927db4c2a",
            "e1522862370c47a89fc987702b2af4d7",
            "f2464cf775cb43cb81414dade5743474",
            "940aee2ae8bb49f093d128d13e880b39",
            "dbdfc30a750d4911b4bd0fa390bb94de",
            "09f8f260bd5a44809031d79829a7a2f6",
            "aa64c199d6bb442db4f7ec93cc9f9c88",
            "5018765cb9cf473a876d84276be542d9",
            "d7a39e48ee794a8a8bb28f4d7830a754"
          ]
        },
        "id": "NjE95-KDjryX",
        "outputId": "6eb0cead-c111-40c4-ab84-b633675fb755"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "print(\"Starting with \"+str(len(ppo_trainer.dataloader.dataset))+\" examples...\")\n",
        "\n",
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Get response from the policy model\n",
        "    response_tensors = []\n",
        "    for query in query_tensors:\n",
        "        gen_len = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "        response_tensors.append(response.squeeze()[-gen_len:])\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "    # Compute sentiment score # noqa\n",
        "    texts = batch[\"response\"]\n",
        "    toxicity_inputs = toxicity_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(\n",
        "        ppo_trainer.accelerator.device\n",
        "    )\n",
        "    #We tried various reward functions, including the softmax of the \n",
        "    #label “neutral,” the log of the toxicity score, and the raw logits of \n",
        "    #the label “neutral.” We found that the convergence was smoother with the \n",
        "    #raw logits of the label “neutral.”\n",
        "    #https://medium.com/@ben.burtenshaw/using-transformer-reinforcement-learning-to-detoxify-generative-language-models-5198446d6786\n",
        "    logits = toxicity_model(**toxicity_inputs).logits.float()\n",
        "    toxicity_labels = (logits[:, 0]).tolist()\n",
        "\n",
        "    rewards = [torch.tensor(output) for output in toxicity_labels]\n",
        "\n",
        "    # Run PPO step\n",
        "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "    # Save model every 100 epochs\n",
        "    if epoch % 2 == 0:\n",
        "        if ppo_trainer.accelerator.is_main_process:\n",
        "            ppo_trainer.save_pretrained(model_save_path)\n",
        "            if(epoch>5):\n",
        "               break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4YQ7XjWSP-M",
        "outputId": "2840e656-e615-4cb2-caa7-1d7f30c87007"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 8.58 µs\n",
            "Starting with 28087 examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6it [38:51, 388.61s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Haphazard test to see if toxicity has improved with RLHF\n",
        "# Feel free to play with generation parameters, the model here has not been fully trained.\n",
        "prompt = (\n",
        "    \"I hate you! You \"\n",
        ")\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "ref_model.to(\"cuda\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# Baseline model\n",
        "gen_tokens = ref_model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=50,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "print(\"Baseline model output:\")\n",
        "print(tokenizer.batch_decode(gen_tokens)[0])\n",
        "\n",
        "# Improved less toxic model\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=50,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "print(\"\\n\\nImproved model output:\")\n",
        "print(tokenizer.batch_decode(gen_tokens)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH5El9e0h7hd",
        "outputId": "102ffa83-d7bc-4953-e9ad-4f5d83085672"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model output:\n",
            "I hate you! You ********! You ********! you ********! YOU ********! YOU! You! YOU! You! YOU! YOU! YOU! YOU! YOU! YOU! YOU!' \r\n",
            "     \n",
            "\n",
            "\n",
            "Improved model output:\n",
            "I hate you! You iz so weak!\n",
            "\n",
            "The man has a strong head, but he is strong, he has a strong head, but you are so weak.\n",
            "\n",
            "But I can't remember one time when in my life I\n"
          ]
        }
      ]
    }
  ]
}